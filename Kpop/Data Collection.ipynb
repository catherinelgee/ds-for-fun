{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Websites for Data and Cleaning\n",
    "So one of my interests is korean pop music (currently taking the world by storm), and I think there are some interesting trend that can be observed so the following are some simple visualization of kpop data. This involves some new concepts I have never tried before in terms of acquiring data. This notebook will include my scraping of data from the gaon charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to scrape data from: gaon, spotify, \n",
    "import requests\n",
    "url = \"http://www.gaonchart.co.kr/main/section/chart/online.gaon?nationGbn=T&serviceGbn=ALL\"\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "soup = bs4.BeautifulSoup(page.content, 'lxml')\n",
    "table = soup.find(name='table')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame([[td.text for td in row.findAll('td')] for row in table.findChildren('tr')])\n",
    "#print(table.findChildren(['tr']))\n",
    "result.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = r'\\W+'\n",
    "col_names = [re.sub(pt, '', th.string) for th in table.find('tr')]\n",
    "result = result.drop([1,2,4,6,7], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict = dict(zip(result.columns, col_names[1:3]+[col_names[4]]))\n",
    "result = result.rename(columns=col_dict)\n",
    "result = result.drop(0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text within the table \n",
    "cleaned_result = result.apply(lambda x: x.apply(lambda y: re.sub(r'\\n', ' ', y)))\n",
    "alter = result['TitleArtist'].apply(lambda x: re.sub(r'\\n', 'newline ', x))\n",
    "\n",
    "cleaned_result['Album'] = result['TitleArtist'].apply(lambda x: re.search(r'\\|(.*)$', x)[1])\n",
    "cleaned_result['Artist'] = result['TitleArtist'].apply(lambda x: re.search(r'(.*)\\|', x)[1])\n",
    "cleaned_result['Title'] = alter.apply(lambda x: re.search(r'(?<=newline).*?(?=newline)', x)[0])\n",
    "cleaned_result['Week'] = 31\n",
    "cleaned_result['Year'] = 2019\n",
    "cleaned_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source a web scraping article I found on Towards Data Science\n",
    "# https://towardsdatascience.com/a-short-practical-how-to-guide-to-scrape-data-from-a-website-using-python-888373227d4f\n",
    "\n",
    "PATH = os.path.join(\"Users\",\"catherinegee\",\"Desktop\",\"cs misc\",\"ds-for-fun\") # you need to change to your local path\n",
    "res = pd.DataFrame()\n",
    "\n",
    "end = '&targetTime=31&hitYear=2019&termGbn=week'\n",
    "groups = ['&targetTime=', '&hitYear=', '&termGbn=week']\n",
    "\n",
    "opts = soup.find(name='select', attrs={'id':'chart_week_select'})\n",
    "lst = []\n",
    "for c in opts.children:\n",
    "    lst += [c]\n",
    "    \n",
    "num_times = len(lst) - 2\n",
    "pth = '/Users/catherinegee/Desktop/cs misc/ds-for-fun/Kpop/data/table.csv'\n",
    "cleaned_result.to_csv(pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(result, table, week, year):\n",
    "    \"\"\"Cleans the table as done in example dataframe\"\"\"\n",
    "    pt = r'\\W+'\n",
    "    col_names = [re.sub(pt, '', th.string) for th in table.find('tr')]\n",
    "    if year <= 2017:\n",
    "        result = result.drop([1,2,5,6], axis=1)\n",
    "        col_dict = dict(zip(result.columns, col_names[1:4]))\n",
    "    else:\n",
    "        result = result.drop([1,2,4,6,7], axis=1)\n",
    "        col_dict = dict(zip(result.columns, col_names[1:3]+[col_names[4]]))\n",
    "    result = result.rename(columns=col_dict)\n",
    "    result = result.drop(0)\n",
    "    cleaned_result = result.apply(lambda x: x.apply(lambda y: re.sub(r'\\n', ' ', y)))\n",
    "    alter = result['TitleArtist'].apply(lambda x: re.sub(r'\\n', 'newline ', x))\n",
    "\n",
    "    cleaned_result['Album'] = result['TitleArtist'].apply(lambda x: re.search(r'\\|(.*)$', x)[1])\n",
    "    cleaned_result['Artist'] = result['TitleArtist'].apply(lambda x: re.search(r'(.*)\\|', x)[1])\n",
    "    cleaned_result['Title'] = alter.apply(lambda x: re.search(r'(?<=newline).*?(?=newline)', x)[0])\n",
    "    cleaned_result['Week'] = week\n",
    "    cleaned_result['Year'] = year\n",
    "    return cleaned_result\n",
    "    \n",
    "def table_to_df(t, week, year): \n",
    "    \"\"\"Turns the table into a datafram using pandas\"\"\"\n",
    "    result = pd.DataFrame([[td.text for td in row.findAll('td')] for row in t.findChildren('tr')])\n",
    "    return clean_df(result, t, week, year)\n",
    "\n",
    "def update_date(week, year):\n",
    "    \"\"\"Updates the date by week number and year\"\"\"\n",
    "    year = int(year)\n",
    "    week = int(week) - 1\n",
    "    if week == 0:\n",
    "        year -= 1\n",
    "        week = 52\n",
    "    elif week < 10:\n",
    "        week = '0' + str(week)\n",
    "    #year = str(year)\n",
    "    return int(week), year, groups[0] + str(week) + groups[1] + str(year) + groups[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_times):\n",
    "    #print(i)\n",
    "    week_n = re.search(r'(?<=&targetTime=).*?(?=&hitYear=)', end)[0] \n",
    "    year = re.search(r'(?<=&hitYear=).*?(?=&termGbn=week)', end)[0] \n",
    "    w, y, end = update_date(week_n, year)\n",
    "    print(w,y,end)\n",
    "    url_week = url + end\n",
    "    pg = requests.get(url_week)\n",
    "    sp = bs4.BeautifulSoup(pg.content, 'lxml')\n",
    "    table = sp.find(name='table')\n",
    "    df1 = table_to_df(table, w, y)\n",
    "    res = res.append(df1)\n",
    "    if i % 20 == 0:\n",
    "        res.to_csv(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = url + '&targetTime=52&hitYear=2017&termGbn=week'\n",
    "pg = requests.get(test)\n",
    "sp = bs4.BeautifulSoup(pg.content, 'lxml')\n",
    "table = sp.find(name='table')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame([[td.text for td in row.findAll('td')] for row in table.findChildren('tr')])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [re.sub(pt, '', th.string) for th in table.find('tr')]\n",
    "col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
